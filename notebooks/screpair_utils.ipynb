{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warn about refactor\n",
    "import warnings\n",
    "warnings.warn(\"Use the refactored 'screpairutils' module instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literals\n",
    "\n",
    "*regarding input files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these library types specify which data we can expect per row\n",
    "DAMID_LIBTYPES = {\"DamID\", \"ChICandDamID\", \"DamIDandT\", \"Damaris\"}\n",
    "CHIC_LIBTYPES = {\"ChIC\", \"ChICandDamID\"}\n",
    "CELSEQ_LIBTYPES = {\"DamIDandT\"}\n",
    "DAMARIS_LIBTYPES = {\"Damaris\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DAMID_POS_FNFMT = \"./experiments/{datadir}/data/counts/{limsid}.index{indexnr:02d}.{barcode}.event_counts.pos.hdf5\"\n",
    "DAMID_BINNED_FNFMT = \"./experiments/{datadir}/data/counts/{limsid}.index{indexnr:02d}.{barcode}.event_counts.binsize_{binsize:d}.hdf5\"\n",
    "DAMID_BINNED_FNFMT_OLDPOP = \"./experiments/{datadir}/data/counts/{limsid}.index{indexnr:02d}.{barcode}.counts.binsize_{binsize:d}.hdf5\"\n",
    "DAMID_OLDPOPFMT_LIMSIDS = {\"KIN1554\", \"KIN1555\", \"KIN1583\", \"KIN1726\"}\n",
    "\n",
    "TX_FNFMT = \"./experiments/{datadir}/data/counts/{limsid}.index{indexnr:02d}.{barcode}.counts.hdf5\"\n",
    "\n",
    "DAMARIS_FNFMT = \"./experiments/{datadir}/data/counts/{limsid}.index{indexnr:02d}.{barcode}.invalid_pos_reads.counts.hdf5\"\n",
    "\n",
    "# try multiple, in order:\n",
    "CHIC_BINNED_FNFMTS = [\n",
    "    \"./experiments/{datadir}/data/counts/{limsid}.index{indexnr:02d}.{barcode}.chic.event_counts.binsize_{binsize:d}.hdf5\",\n",
    "    \"./experiments/{datadir}/data/counts/{limsid}.index{indexnr:02d}.{barcode}.chic.counts.binsize_{binsize:d}.hdf5\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BARCODENAMEFMTS = {\n",
    "    \"unspecified\": \"BC_{barcodenr:03d}\",\n",
    "    \"damid\": \"DamID_BC_{barcodenr:03d}\",\n",
    "    \"damid2\": \"DamID2_BC_{barcodenr:03d}\",\n",
    "    \"damid_v2\": \"BC_DamIDv2_{barcodenr:03d}\",\n",
    "    \"damid_v3\": \"BC_DamIDv3_{barcodenr:03d}\",\n",
    "    \"damid_v3_set1\": \"BCv3set1_BC_{barcodenr:03d}\",\n",
    "    \"damid_v3_set2\": \"BCv3set2_BC_{barcodenr:03d}\",\n",
    "    \"chic\": \"BC_ChIC_{barcodenr:03d}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset from HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_with_dim(f, name, shape, dtype=None):\n",
    "    if not name in f:\n",
    "        return np.zeros(shape, dtype=(int if dtype is None else dtype))\n",
    "    else:\n",
    "        d = f[name][:]\n",
    "        if d.shape == shape:\n",
    "            if dtype is None:\n",
    "                return d\n",
    "            else:\n",
    "                return d.astype(dtype)\n",
    "\n",
    "        if len(shape) > d.ndim:\n",
    "            raise ValueError(\"Could be implemented with `np.pad` but I'm lazy\")\n",
    "\n",
    "        if d.ndim > len(shape):\n",
    "            reduceaxes = d.ndim - len(shape)\n",
    "            d = np.add.reduce(d, -reduceaxes)\n",
    "\n",
    "        diff = shape[0] - len(d)\n",
    "        if diff > 0:\n",
    "            d = np.pad(d, (0, diff), mode='constant', constant_values=d.dtype.type())\n",
    "        elif diff < 0:\n",
    "            d = d[:shape[0]]\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_gaussian_window(s):\n",
    "    s = float(s)\n",
    "    W = scipy.signal.windows.gaussian(int(np.ceil(3 * s)) * 2 + 1, s)\n",
    "    return (W / W.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOOTH_STDDEV_SF = 2500e3\n",
    "\n",
    "W_sf = normalized_gaussian_window(SMOOTH_STDDEV_SF / BINSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-convolve mappability for the sake of SF calculations\n",
    "binned_mapab_c = {\n",
    "    chrom: scipy.signal.fftconvolve(binned_mapab[chrom].astype(float), W_sf, mode='same')\n",
    "    for chrom in chroms\n",
    "}\n",
    "\n",
    "total_mapab = sum(int(binned_mapab[chrom].sum()) for chrom in chroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scale_factor(X, alpha=0.5):\n",
    "    \"\"\"\n",
    "    X: (Nbins, Nsamples)\n",
    "    \"\"\"\n",
    "    \n",
    "    _offset = 0.01\n",
    "    \n",
    "    assert X.ndim == 2\n",
    "    assert (X.sum(axis=0) > 0).all()\n",
    "    \n",
    "    w = ((~np.isclose(X, 0)) & (X > 0)).any(axis=1)\n",
    "    assert w.ndim == 1\n",
    "    \n",
    "    assert w.sum() > 0\n",
    "    \n",
    "    Xij = X[w]\n",
    "    S = np.ones(len(Xij), dtype=bool)\n",
    "    \n",
    "    Xi = Xij.sum(axis=1)\n",
    "    sj = Xij[S].sum(axis=0) / Xi[S].sum(axis=0)\n",
    "    \n",
    "    assert np.isclose(sj.sum(), 1.)\n",
    "    \n",
    "    Eij = np.outer(Xi, sj)\n",
    "    GOFi = (((Xij - Eij) ** 2) / Eij).sum(axis=1)\n",
    "    assert len(GOFi) == len(S)\n",
    "    \n",
    "    _GOF_low, _GOF_high = np.quantile(GOFi, np.array([_offset, min(1.0 - _offset, 1 - alpha + _offset)]))\n",
    "    \n",
    "    for _ in range(20):\n",
    "        S_update = (GOFi >= _GOF_low) & (GOFi < _GOF_high)\n",
    "        if (S == S_update).all():\n",
    "            break\n",
    "\n",
    "        S = S_update.copy()\n",
    "\n",
    "        sj = Xij[S].sum(axis=0) / Xi[S].sum(axis=0)\n",
    "        \n",
    "        Eij = np.outer(Xi, sj)\n",
    "        GOFi = (((Xij - Eij) ** 2) / Eij).sum(axis=1)\n",
    "        \n",
    "        _GOF_low, _GOF_high = np.quantile(GOFi, np.array([_offset, min(1.0 - _offset, 1 - alpha + _offset)]))\n",
    "    else:\n",
    "        warnings.warn(\"Did not converge\")\n",
    "    \n",
    "    sj_final = sj.copy()\n",
    "    \n",
    "    return sj_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scale_factor_adapt(X, alpha=0.5):\n",
    "    \"\"\"\n",
    "    X: (Nbins, Nsamples)\n",
    "    \"\"\"\n",
    "    \n",
    "    _offset = 0.01\n",
    "    \n",
    "    assert X.ndim == 2\n",
    "    assert (X.sum(axis=0) > 0).all()\n",
    "    \n",
    "    w = ((~np.isclose(X, 0)) & (X > 0)).any(axis=1)\n",
    "    assert w.ndim == 1\n",
    "    \n",
    "    assert w.sum() > 0\n",
    "    \n",
    "    Xij = X[w]\n",
    "    S = np.ones(len(Xij), dtype=bool)\n",
    "    \n",
    "    Xi = Xij.sum(axis=1)\n",
    "    sj = Xij[S].sum(axis=0) / Xi[S].sum(axis=0)\n",
    "    \n",
    "    assert np.isclose(sj.sum(), 1.)\n",
    "    \n",
    "    Eij = np.outer(Xi, sj)\n",
    "    # Note: difference instead of squared distance!\n",
    "    GOFi = ((Xij - Eij) / Eij).sum(axis=1)\n",
    "    assert len(GOFi) == len(S)\n",
    "    \n",
    "    _GOF_low, _GOF_high = np.quantile(GOFi, np.array([0.01, min(1.0 - 0.01, 1 - alpha + 0.01)]))\n",
    "    \n",
    "    for _ in range(20):\n",
    "        S_update = (GOFi >= _GOF_low) & (GOFi < _GOF_high)\n",
    "        if (S == S_update).all():\n",
    "            break\n",
    "\n",
    "        S = S_update.copy()\n",
    "        sj = Xij[S].sum(axis=0) / Xi[S].sum(axis=0)\n",
    "        \n",
    "        Eij = np.outer(Xi, sj)\n",
    "        GOFi = (((Xij - Eij) ** 1) / Eij).sum(axis=1)\n",
    "\n",
    "        _GOF_low, _GOF_high = np.quantile(GOFi, np.array([0.01, min(1.0 - 0.01, 1 - alpha + 0.01)]))\n",
    "    else:\n",
    "        warnings.warn(\"Did not converge\")\n",
    "    \n",
    "    sj_final = sj.copy()\n",
    "    \n",
    "    return sj_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function:\n",
    "def calc_sf(fg, bg, w_mapab, alpha):\n",
    "    total_bg = float(sum(v.sum() for v in bg.values()))\n",
    "    assert fg.keys() == bg.keys()\n",
    "    assert set(w_mapab.keys()).issuperset(set(fg.keys()))\n",
    "    \n",
    "    chroms = sorted(fg.keys())\n",
    "    \n",
    "    fg_c = {chrom: scipy.signal.fftconvolve(fg[chrom].astype(float), W_sf, mode='same') for chrom in chroms}\n",
    "    bg_c = {chrom: scipy.signal.fftconvolve(bg[chrom].astype(float), W_sf, mode='same') for chrom in chroms}\n",
    "    \n",
    "    Xij = np.array([\n",
    "        np.concatenate([fg_c[chrom][w_mapab[chrom]] for chrom in chroms]),\n",
    "        np.concatenate([bg_c[chrom][w_mapab[chrom]] for chrom in chroms]),\n",
    "    ]).T\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore')\n",
    "        rel_sf = calc_scale_factor(Xij, alpha=alpha)\n",
    "\n",
    "    assert np.isclose(1., rel_sf.sum())\n",
    "\n",
    "    sf = rel_sf[0] / (rel_sf[1] / total_bg)  # scale fg to bg\n",
    "    assert sf > 0.\n",
    "    \n",
    "    return sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "difference, distance (absolute difference) and squared, visualized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = np.meshgrid(np.arange(1, 10), np.arange(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = (a - b) / b\n",
    "\n",
    "# plt.pcolormesh(minmax_scale(y.ravel()).reshape(y.shape), cmap=\"coolwarm\", vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = np.abs(a - b) / b\n",
    "\n",
    "# plt.pcolormesh(minmax_scale(y.ravel()).reshape(y.shape), cmap=\"coolwarm\", vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = ((a - b) ** 2) / b\n",
    "\n",
    "# plt.pcolormesh(minmax_scale(y.ravel()).reshape(y.shape), cmap=\"coolwarm\", vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load AsiSI site tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASISI_SITE_FN = \"/data/zfs/deepseq/projects/DNADamage/KR20160411/data/regions/Homo_sapiens.GRCh37.dna.primary_assembly.AsiSI_GCGATCGC.bed\"\n",
    "\n",
    "TOP_SITE_FN = \"/data/zfs/deepseq/projects/DNADamage/KR20170201.AsiSI_top_sites/output/KR20170202.write_breakseq_chipseq_defined_asisi_tophit_subset/KR20170203.BREAkseq_and_gH2AX_defined_AsiSI_top_sites.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asisi_sites = pd.read_csv(ASISI_SITE_FN, sep=\"\\t\", header=None, names=[\"chrom\", \"start\", \"end\"])\n",
    "# NOTE: asisi sites are also listed on contigs (GL.. etc); remove those entries\n",
    "asisi_sites = asisi_sites[asisi_sites[\"chrom\"].isin(chroms)].reset_index(drop=True)\n",
    "\n",
    "top_sites = pd.read_csv(TOP_SITE_FN, sep=\"\\t\", header=None, usecols=[0, 1, 2], names=[\"chrom\", \"start\", \"end\"])\n",
    "\n",
    "assert ((asisi_sites[\"end\"] - asisi_sites[\"start\"]) == 8).all()\n",
    "\n",
    "assert ((top_sites[\"end\"] - top_sites[\"start\"]) == 8).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_prob(v, n, random_seed=None):\n",
    "    \"\"\"\n",
    "    Probablistic subsampling\n",
    "    \"\"\"\n",
    "    assert v.ndim == 1\n",
    "    vs = v.sum()\n",
    "    assert n <= vs\n",
    "    rs = np.random.RandomState(random_seed)\n",
    "    return rs.binomial(n, v / vs)\n",
    "\n",
    "\n",
    "def subsample_chroms_prob(ds, n, random_seed=None):\n",
    "    # transparently concatenate chroms and re-split after subsampling\n",
    "    dschroms = sorted(ds)\n",
    "    v = np.concatenate([ds[chrom] for chrom in dschroms])\n",
    "    vs = subsample_prob(v, n, random_seed)\n",
    "    dschrompos = np.array([ds[chrom].size for chrom in dschroms]).cumsum()[:-1]\n",
    "    return dict(zip(dschroms, np.split(vs, dschrompos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def negate_chroms(vs):\n",
    "#     return {chrom: ~vs[chrom] for chrom in vs}\n",
    "\n",
    "# embrace the functional approach:\n",
    "from functools import reduce\n",
    "\n",
    "def map_dict(func, *args):\n",
    "    keys = reduce(set.intersection, (set(d.keys()) for d in args))\n",
    "    return {k: func(*[d[k] for d in args]) for k in keys}\n",
    "# --> negate_chroms(vs) =~ map_dict(np.logical_not, vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_ds(ds, w, fillvalue=0):\n",
    "    return np.choose(\n",
    "        w,\n",
    "        (ds, fillvalue)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barcode_from_row(row):\n",
    "    return BARCODENAMEFMTS[row[\"genomic_barcodetype\"]].format(barcodenr=row[\"barcodenr\"])\n",
    "\n",
    "\n",
    "def damid_fn_from_row(row):\n",
    "    barcode = barcode_from_row(row)\n",
    "    \n",
    "    if (\n",
    "        (row[\"limsid\"] in DAMID_OLDPOPFMT_LIMSIDS)\n",
    "        and (row[\"cellcount\"] > 16)\n",
    "    ):\n",
    "        fn = DAMID_BINNED_FNFMT_OLDPOP.format(**row.to_dict(), barcode=barcode, binsize=BINSIZE)\n",
    "    else:\n",
    "        fn = DAMID_BINNED_FNFMT.format(**row.to_dict(), barcode=barcode, binsize=BINSIZE)\n",
    "    \n",
    "    return fn\n",
    "\n",
    "\n",
    "def chic_fn_from_row(row):\n",
    "    barcode = barcode_from_row(row)\n",
    "    \n",
    "    # deal with multiple output file formats :/\n",
    "    fns = map(\n",
    "        lambda fnfmt: fnfmt.format(**row.to_dict(), barcode=barcode, binsize=BINSIZE),\n",
    "        CHIC_BINNED_FNFMTS,\n",
    "    )\n",
    "    fn = next(fn for fn in fns if os.access(fn, os.R_OK))\n",
    "    \n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_binned_ds(fn, binned_chromsizes):\n",
    "    chroms = binned_chromsizes.keys()\n",
    "    \n",
    "    with h5py.File(fn, 'r') as f:\n",
    "        ds = {\n",
    "            chrom: get_dataset_with_dim(f, chrom, (binned_chromsizes[chrom], ))\n",
    "            for chrom in chroms\n",
    "        }\n",
    "        \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
